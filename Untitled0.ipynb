{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xIckrfN649f0SoBeneMAPxS0L0qOSPbL",
      "authorship_tag": "ABX9TyPSt1+bz6XrI+6VEfKeT7zi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzjcaf/colab/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "OyyRKciUPuxM"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install segmentation-models-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6req8cslBH2",
        "outputId": "2ec4487e-b598-419a-8258-cd51e8d825cf"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.12.0+cu113)\n",
            "Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.4.12)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.11.0+cu113)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = [64,64]\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),   # 这一步取决于后续的数据读取方式，如果使用内置数据集则不需要\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "fuY-0VxAC2PO"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mydataset(Dataset):\n",
        "  def __init__(self,root,img_data,label_data,transform):\n",
        "    self.root_dir = root\n",
        "    self.img_dir = img_data\n",
        "    self.label_dir = label_data\n",
        "    self.img_path = os.path.join(self.root_dir,self.img_dir)\n",
        "    self.label_path = os.path.join(self.root_dir,self.label_dir)\n",
        "    self.img_list = os.listdir(self.img_path)\n",
        "    self.label_list = os.listdir(self.label_path)\n",
        "    self.transforms = transform\n",
        "    self.img_list.sort()\n",
        "    self.label_list.sort()\n",
        "  def __getitem__(self,idx):\n",
        "    img_name = self.img_list[idx]\n",
        "    label_name = self.label_list[idx]\n",
        "    img_idx_path = os.path.join(self.root_dir,self.img_dir,img_name)\n",
        "    label_idx_path = os.path.join(self.root_dir,self.label_dir,label_name)\n",
        "    #图像\n",
        "    img = Image.open(img_idx_path)\n",
        "    img = np.array(img)\n",
        "    img = self.transforms(img)\n",
        "    #标签\n",
        "    label = Image.open(label_idx_path)\n",
        "    label = np.array(label)\n",
        "    label = self.transforms(label)\n",
        "    return img,label\n",
        "  def __len__(self):\n",
        "    #assert len(self.img_list) == len(self.label_list)\n",
        "    return len(self.img_list)"
      ],
      "metadata": {
        "id": "IS602n45k9AS"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/drive/MyDrive/data\"\n",
        "lab = \"lab\"\n",
        "img = \"img\""
      ],
      "metadata": {
        "id": "T-J_ZYdJtfx8"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Mydataset(root,img,lab,data_transform)"
      ],
      "metadata": {
        "id": "YdyVdgvJt2Gy"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,(img,label) in enumerate(dataset,0):\n",
        "  print(i,img.shape,label.shape)"
      ],
      "metadata": {
        "id": "wt-dufPU3YwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472d0a99-9642-4dba-cc52-43797f311087"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "1 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "2 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "3 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "4 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "5 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "6 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "7 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "8 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "9 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n",
            "10 torch.Size([3, 64, 64]) torch.Size([1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=2\n",
        "train_dataset = DataLoader(dataset,batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "bJm2GMVsvvQt"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "    classes=1,                      # model output channels (number of classes in your dataset)\n",
        ")\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model.to(device)"
      ],
      "metadata": {
        "id": "2aRIy9FsRDZ2"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "v6bYFEvBo-2P"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  train_loss = 0.0\n",
        "  for batch_idx, (img,label) in enumerate(train_dataset, 0):\n",
        "    #img,label = img.to(device),label.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    # forward + backward + update\n",
        "    outputs = model(img)\n",
        "    #print(outputs.size(),label.size())\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*img.size(0)\n",
        "  train_loss = train_loss/len(train_dataset.dataset)\n",
        "  print(epoch,train_loss)"
      ],
      "metadata": {
        "id": "4LjjfoRpEiyQ"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "  train(epoch)"
      ],
      "metadata": {
        "id": "UNIFh-GoTJQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3038ec-571a-4684-a438-32428f01a6e8"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.0\n",
            "1 0.0\n",
            "2 0.0\n"
          ]
        }
      ]
    }
  ]
}